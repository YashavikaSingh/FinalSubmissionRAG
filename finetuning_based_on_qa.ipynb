{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install bitsandbytes\n"
      ],
      "metadata": {
        "id": "xknUm-VbpiVq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %%capture\n",
        "# import torch\n",
        "# major_version, minor_version = torch.cuda.get_device_capability()\n",
        "\n",
        "# # Must install separately since Colab has torch 2.2.1, which breaks packages\n",
        "# !pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
        "# if major_version >= 8:\n",
        "#     # Use this for new GPUs like Ampere, Hopper GPUs (RTX 30xx, RTX 40xx, A100, H100, L40)\n",
        "#     !pip install --no-deps packaging ninja einops flash-attn xformers trl peft accelerate bitsandbytes\n",
        "# else:\n",
        "#     # Use this for older GPUs (V100, Tesla T4, RTX 20xx)\n",
        "#     !pip install --no-deps xformers trl peft accelerate bitsandbytes\n"
      ],
      "metadata": {
        "id": "ZN6yPAdQof5N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n"
      ],
      "metadata": {
        "id": "qGwKVNK8o9x0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "vNGumq4srEAc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "id": "fnf7PpB4zN6y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install unsloth"
      ],
      "metadata": {
        "id": "MEpCF8ytzU3x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install huggingface_hub"
      ],
      "metadata": {
        "id": "rILIghAm2XTZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive (skip this if already mounted)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Load the JSON file from Google Drive\n",
        "import json\n",
        "json_file_path = '/content/drive/MyDrive/qna_triplets.json'\n",
        "\n",
        "with open(json_file_path, 'r') as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "# Check the structure of your data (optional)\n",
        "# print(data[:1])  # Print the first item to check structure\n",
        "\n",
        "# Convert the list of dictionaries to a Hugging Face Dataset\n",
        "from datasets import Dataset\n",
        "\n",
        "# Since 'data' is a list of dictionaries, we can use Dataset.from_dict\n",
        "dataset = Dataset.from_pandas(pd.DataFrame(data))\n",
        "\n",
        "from unsloth import FastLanguageModel\n",
        "\n",
        "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
        "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
        "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/llama-2-7b-bnb-4bit\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        ")\n",
        "\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r=16,  # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                    \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0,\n",
        "    bias=\"none\",\n",
        "    use_gradient_checkpointing=\"unsloth\",\n",
        "    random_state=3407,\n",
        "    use_rslora=False,\n",
        "    loftq_config=None,\n",
        ")\n",
        "\n",
        "# Define your prompt template\n",
        "alpaca_prompt = \"\"\"Below is a question paired with a context. Write an answer based on the context.\n",
        "\n",
        "### Question:\n",
        "{}\n",
        "\n",
        "### Context:\n",
        "{}\n",
        "\n",
        "### Answer:\n",
        "{}\"\"\"\n",
        "\n",
        "EOS_TOKEN = tokenizer.eos_token  # Set the EOS token\n",
        "\n",
        "# Formatting function to convert data to the correct prompt format\n",
        "def formatting_prompts_func(examples):\n",
        "    questions = examples[\"question\"]\n",
        "    contexts = examples[\"context\"]\n",
        "    answers = examples[\"answer\"]\n",
        "    texts = []\n",
        "    for question, context, answer in zip(questions, contexts, answers):\n",
        "        text = alpaca_prompt.format(question, context, answer) + EOS_TOKEN\n",
        "        texts.append(text)\n",
        "    return {\"text\": texts}\n",
        "\n",
        "# Apply the formatting function to your dataset\n",
        "dataset = dataset.map(formatting_prompts_func, batched=True)\n",
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "\n",
        "# Define the trainer and training arguments\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=dataset,\n",
        "    dataset_text_field=\"text\",\n",
        "    max_seq_length=max_seq_length,\n",
        "    dataset_num_proc=2,\n",
        "    args=TrainingArguments(\n",
        "        per_device_train_batch_size=2,\n",
        "        run_name=\"run1\",\n",
        "        gradient_accumulation_steps=4,\n",
        "        warmup_steps=5,\n",
        "        max_steps=20,\n",
        "        learning_rate=2e-4,\n",
        "        fp16=not torch.cuda.is_bf16_supported(),\n",
        "        bf16=torch.cuda.is_bf16_supported(),\n",
        "        logging_steps=1,\n",
        "        optim=\"adamw_8bit\",\n",
        "        weight_decay=0.01,\n",
        "        lr_scheduler_type=\"linear\",\n",
        "        seed=3407,\n",
        "        output_dir=\"outputs\",\n",
        "    ),\n",
        ")\n",
        "\n",
        "# Start the training\n",
        "trainer_stats = trainer.train()\n"
      ],
      "metadata": {
        "id": "_WPn87vZUXlM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login\n"
      ],
      "metadata": {
        "id": "XARVSZknqxr6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48070942-9ba1-47d3-b4c3-6d5d85e62e65"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
            "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
            "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
            "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
            "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
            "\n",
            "    To log in, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
            "Enter your token (input will not be visible): "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained_gguf(\"model\", tokenizer, quantization_method = \"f16\")\n"
      ],
      "metadata": {
        "id": "yDuVSQnxAmpd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "access_token = \"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\"\n",
        "model.push_to_hub_gguf(\n",
        "    \"debika/model\",\n",
        "    tokenizer=tokenizer,\n",
        "    quantization_method=\"f16\",\n",
        "    token=access_token\n",
        ")\n"
      ],
      "metadata": {
        "id": "F6hf7BSakY_h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model.save_pretrained_gguf(\"model\", tokenizer, quantization_method = \"q4_k_m\")\n",
        "# access_token = \"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\"\n",
        "# model.push_to_hub_gguf(\n",
        "#     \"debika/model_final\",\n",
        "#     tokenizer=tokenizer,\n",
        "#     quantization_method=\"q4_k_m\",\n",
        "#     token=access_token\n",
        "# )"
      ],
      "metadata": {
        "id": "E3XW33J3le2u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Define paths\n",
        "# model_save_path = '/content/drive/MyDrive/llama_7b_model_version2'\n",
        "# tokenizer_save_path = '/content/drive/MyDrive/llama_7b_tokenizer_version2'\n",
        "# hf_model_repo = \"debika/model_ig\"\n",
        "\n",
        "# # Save merged model and tokenizer\n",
        "# model.save_pretrained_merged(model_save_path, tokenizer, save_method=\"merged_16bit\")\n",
        "# tokenizer.save_pretrained(tokenizer_save_path)\n",
        "\n",
        "\n",
        "# # Save to 16bit GGUF\n",
        "\n",
        "\n",
        "# print(\"Model and tokenizer saved locally in merged format.\")"
      ],
      "metadata": {
        "id": "d91_twJmZbaN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3j4aU2gxdw_e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "E2j3bO8Vdw0J"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}