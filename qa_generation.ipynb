{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing chunked data list from other file\n",
    "from new_try_latest import  chunked_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# choosing random 1000 chunks to generate questions from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "chunked_sample=random.sample(chunked_data,1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client import QdrantClient\n",
    "\n",
    "# Initialize Qdrant Client\n",
    "qdrant_client = QdrantClient(\"http://localhost:6333\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def retrieve_similar_documents(query_embedding, collection_name=\"unified_collection\", top_k=5):\n",
    "    \"\"\"\n",
    "    Retrieve the top-k most similar documents from Qdrant for the given query embedding.\n",
    "    \"\"\"\n",
    "    if not query_embedding or not isinstance(query_embedding, list):\n",
    "        print(\"Invalid query embedding. Ensure it is a list of floats.\")\n",
    "        return [], [], \"\"\n",
    "\n",
    "    results = qdrant_client.search(\n",
    "        collection_name=collection_name,\n",
    "        query_vector=query_embedding,\n",
    "        limit=top_k  # Retrieve the top-k matches\n",
    "    )\n",
    "\n",
    "    # Extract relevant information from results\n",
    "    similar_docs = [\n",
    "        {\n",
    "            \"content\": result.payload.get(\"content\"),  # The actual text content\n",
    "            \"metadata\": {\n",
    "                \"type\": result.payload.get(\"type\"),\n",
    "                \"url\": result.payload.get(\"url\")\n",
    "            },\n",
    "            \"score\": result.score  # Similarity score\n",
    "        }\n",
    "        for result in results\n",
    "    ]\n",
    "    context_chunks = [result.payload for result in results]\n",
    "    context = \"\\n\".join([result.payload[\"content\"] for result in results])\n",
    "\n",
    "    return similar_docs, context_chunks, context\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_completion(completion, batch):\n",
    "    results = []\n",
    "    try:\n",
    "        # Split by '#### Content' to get individual sections\n",
    "        sections = completion.split(\"#### Content\")\n",
    "        \n",
    "        \n",
    "        for i, section in enumerate(sections[1:], start=0):  # Skip the initial empty or header part\n",
    "            lines = section.strip().split(\"\\n\")\n",
    "            current_question = None\n",
    "            current_answer = None\n",
    "            qa_pairs = []\n",
    "\n",
    "            # Extract all Q&A pairs from the section\n",
    "            for line in lines:\n",
    "                if \"**Question:**\" in line:\n",
    "                    # If there is an ongoing Q&A, save it before starting a new one\n",
    "                    if current_question and current_answer:\n",
    "                        qa_pairs.append((current_question, current_answer))\n",
    "                    current_question = line.split(\"**Question:**\")[1].strip()\n",
    "                    current_answer = None  # Reset answer for a new question\n",
    "                elif \"**Answer:**\" in line:\n",
    "                    current_answer = line.split(\"**Answer:**\")[1].strip()\n",
    "\n",
    "            # Add the last Q&A pair in the section\n",
    "            if current_question and current_answer:\n",
    "                qa_pairs.append((current_question, current_answer))\n",
    "\n",
    "            # Map extracted Q&A pairs to the corresponding chunk in the batch\n",
    "            for question, answer in qa_pairs:\n",
    "                if i < len(batch):  # Ensure mapping does not exceed batch size\n",
    "                    _, context_chunks, _ = retrieve_similar_documents(\n",
    "                        query_embedding=batch[i].get(\"embedding\"),  # Assuming batch items have 'embedding'\n",
    "                        collection_name=\"unified_collection\",\n",
    "                        top_k=5\n",
    "                    )\n",
    "                    results.append({\n",
    "                        \"question\": question,\n",
    "                        \"answer\": answer,\n",
    "                        \"context\": context_chunks,\n",
    "                    })\n",
    "        \n",
    "        return results\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing completion: {e}\")\n",
    "        return []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import time\n",
    "import csv\n",
    "import json\n",
    "# Set up OpenAI API Key\n",
    "client = OpenAI(\n",
    "# Set your OpenAI API key\n",
    "api_key= \"xxx\"\n",
    ")\n",
    "BATCH_SIZE = 10\n",
    "\n",
    "# List to store generated triplets\n",
    "triplets = []\n",
    "def generate_qna_batch(batch):\n",
    "    try:\n",
    "        # Concatenate content from all chunks in the batch\n",
    "        concatenated_content = \"\\n\\n\".join([f\"Content {i+1}: {chunk['content']}\" for i, chunk in enumerate(batch)])\n",
    "        messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are an assistant that generates questions and answers for given content.\"},\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": (\n",
    "            \"Based on the following content, generate  question and answer for each section. \"\n",
    "\"Do not limit the number of questions and answers; generate as many as relevant.\\n\\n\"\n",
    "\"### Questions and Answers:\\n\\n\"\n",
    "\"#### Content 1:\\n\"\n",
    "\"**Question:** [Insert question here]\\n\"\n",
    "\"**Answer:** [Insert answer here]\\n\\n\"\n",
    "\n",
    "\n",
    "        ),\n",
    "    },\n",
    "    {\"role\": \"user\", \"content\": f\"{concatenated_content}\"},\n",
    "]\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=messages,\n",
    "            temperature=0.7,\n",
    "        )\n",
    "        # print(\"response is:\",response)\n",
    "        completion = response.choices[0].message.content.strip()\n",
    "        \n",
    "      \n",
    "        # print(\"completion:\",completion)\n",
    "        return parse_completion(completion,batch)\n",
    "      \n",
    "    \n",
    "        \n",
    "   \n",
    "    except Exception as e:\n",
    "        print(f\"Error generating Q&A for batch: {e}\")\n",
    "        return []    \n",
    "\n",
    "    \n",
    "\n",
    "        \n",
    "      \n",
    "\n",
    "# Process each chunk to generate triplets\n",
    "for i in range(0, len(chunked_sample), BATCH_SIZE):\n",
    "    print(f\"Processing data {i}\")\n",
    "    batch = chunked_sample[i:i + BATCH_SIZE]\n",
    "    batch_results=generate_qna_batch(batch)\n",
    "    print(\"batch result is:\",batch_results)\n",
    "    triplets.extend(batch_results)\n",
    "\n",
    "    print(f\"Batch {i // BATCH_SIZE + 1} processed. Generated {len(batch_results)} Q&A pairs.\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_vague_qna(triplets):\n",
    "    filtered_triplets = []\n",
    "    for triplet in triplets:\n",
    "        # Check if the answer is too short or incomplete\n",
    "        if triplet[\"answer\"].endswith(\":\"):\n",
    "            continue  # Skip vague Q&A\n",
    "        \n",
    "       \n",
    "        filtered_triplets.append(triplet)\n",
    "    return filtered_triplets\n",
    "\n",
    "# Apply the filter\n",
    "triplets_now = filter_vague_qna(triplets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_triplets=[]\n",
    "for each in triplets_now:\n",
    "    new_triplets.append({'question':each['question'],'answer':each['answer'],'context':each['context']})\n",
    "with open(\"qna_triplets.json\", \"w\") as json_file:\n",
    "    json.dump(new_triplets, json_file, indent=4)\n",
    "\n",
    "with open(\"qna_triplets_final.csv\", \"w\", newline=\"\") as csv_file:\n",
    "    writer = csv.DictWriter(\n",
    "        csv_file,\n",
    "        fieldnames=[\"question\", \"answer\", \"context\"]\n",
    "    )\n",
    "    writer.writeheader()\n",
    "    writer.writerows(new_triplets)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
